{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "oISl_AJMCYUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "    --extra-index-url=https://pypi.nvidia.com \\\n",
        "    cudf-cu12==23.12.* dask-cudf-cu12==23.12.* cuml-cu12==23.12.* \\\n",
        "    cugraph-cu12==23.12.* cuspatial-cu12==23.12.* cuproj-cu12==23.12.* \\\n",
        "    cuxfilter-cu12==23.12.* cucim-cu12==23.12.* pylibraft-cu12==23.12.* \\\n",
        "    raft-dask-cu12==23.12.*"
      ],
      "metadata": {
        "id": "7fSVCVnakyty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml import svm\n",
        "from cuml import LogisticRegression\n",
        "from cuml.common import logger\n",
        "import datasets\n",
        "import experiments\n",
        "import features\n",
        "import utilities\n",
        "import gcn\n",
        "import node2vec\n",
        "import ensemble\n",
        "import torch\n",
        "from model import model_training\n",
        "import pickle as pk\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler,FunctionTransformer, Normalizer\n",
        "import numpy\n",
        "import gc\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "xx3P0_wSie3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass 'ogbn-arxiv' to load ArXiv dataset\n",
        "G, data = datasets.load_data('cora')\n",
        "print(data)"
      ],
      "metadata": {
        "id": "0iRAueiViqeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: basic GNN + combinations of structural and positional features"
      ],
      "metadata": {
        "id": "oRskOYKY6g0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GCN setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = gcn.GCNBase(data,hidden_channels=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "n_epochs = 200\n",
        "n_runs = 10"
      ],
      "metadata": {
        "id": "1pXmf7nCFzVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global features\n",
        "original_features = data.x.to(device)\n",
        "structural_features = features.structural_features(G,['cc', 'bc', 'dc', 'ec', 'pr', 'cn', 'lc', 'nd', 'kc']).to(device)\n",
        "positional_features = features.positional_features(data,128,50)"
      ],
      "metadata": {
        "id": "N_fB_lms61zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#structural_features=utilities.load_results('structural_features')\n",
        "#positional_features=utilities.load_results('positional_features')"
      ],
      "metadata": {
        "id": "CWhH9lJiZxo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%load_ext autoreload\n",
        "#%autoreload 2"
      ],
      "metadata": {
        "id": "ngCX4ugLcsQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gcn_base_factory(data, hidden_channels):\n",
        "  return gcn.GCNBase(data, hidden_channels)\n",
        "\n",
        "def gcn_pre_factory(data, hidden_channels, mlp_hidden_channels):\n",
        "  return gcn.GCNPre(data, hidden_channels, mlp_hidden_channels)"
      ],
      "metadata": {
        "id": "BJ8tXy0QYkQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute models with all feature combinations, both for base and mlp GCN\n",
        "def run_feature_combinations(file_name, model_factory, model_factory_params, normalization=lambda x: x):\n",
        "    features_combinations = [\n",
        "      original_features,\n",
        "      structural_features,\n",
        "      positional_features,\n",
        "      utilities.concatenate(original_features,structural_features),\n",
        "      utilities.concatenate(original_features,positional_features),\n",
        "      utilities.concatenate(structural_features,positional_features),\n",
        "      utilities.concatenate(original_features,structural_features,positional_features)]\n",
        "\n",
        "    file_names = [\n",
        "      'original',\n",
        "      'structural',\n",
        "      'positional',\n",
        "      'original-structural',\n",
        "      'original-positional',\n",
        "      'structural-positional',\n",
        "      'original-structural-positional']\n",
        "\n",
        "    basic_models = dict()\n",
        "    orig_num_feat = original_features.size()[1]\n",
        "    for curr_features, curr_file_name in zip(features_combinations, file_names):\n",
        "        data.x = curr_features\n",
        "        data.x = normalization(data.x)\n",
        "\n",
        "        if data.name=='Cora' and (curr_file_name=='original' or curr_file_name=='original-structural' or curr_file_name=='original-positional' or curr_file_name=='original-structural-positional'):\n",
        "          split = curr_features.split([orig_num_feat,curr_features.size()[1]-orig_num_feat],dim=-1)\n",
        "          orig_feats = split[0]\n",
        "          other_feats = split[1]\n",
        "          other_feats_norm = normalization(other_feats)\n",
        "          data.x = utilities.concatenate(orig_feats,other_feats_norm)\n",
        "\n",
        "        model = model_factory(data, *model_factory_params)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "        results = dict()\n",
        "        results['avg_acc'], results['test_accs'], results['train_losses'], results['train_accs'], results['val_losses'], results['val_accs'], results['run_times'],results['best_epoch'] = experiments.run_experiments(model, data, n_runs, n_epochs, optimizer, criterion, device) # These should be \"global variables\"\n",
        "\n",
        "        basic_models[curr_file_name] = results\n",
        "\n",
        "    utilities.save_results(basic_models, file_name)"
      ],
      "metadata": {
        "id": "jFKhMWRciBgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_feature_combinations('gcn-base-concatenation-without-norm-cora', gcn_base_factory,[64])\n",
        "gcn_base_concatenation_without_norm_cora = utilities.load_results('gcn-base-concatenation-without-norm-cora')\n",
        "print(gcn_base_concatenation_without_norm_cora)"
      ],
      "metadata": {
        "id": "ydGwUPambpdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Min-Max Normalization"
      ],
      "metadata": {
        "id": "3lbQ-_ktl7oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_max_normalization = lambda x : utilities.MinMaxNormalization(x)\n",
        "run_feature_combinations('gcn-base-concatenation-minmax-norm-cora', gcn_base_factory,[64],normalization = min_max_normalization)\n",
        "gcn_base_concatenation_minmax_norm_cora = utilities.load_results('gcn-base-concatenation-minmax-norm-cora')\n",
        "print(gcn_base_concatenation_minmax_norm_cora)"
      ],
      "metadata": {
        "id": "lGXzj6VYlcKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Standard Normalization"
      ],
      "metadata": {
        "id": "RIVhsMffmBKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standard_normalization = lambda x : utilities.StandardNormalization(x)\n",
        "run_feature_combinations('gcn-base-concatenation-standard-norm-cora', gcn_base_factory,[64],normalization = standard_normalization)\n",
        "gcn_base_concatenation_standard_norm_cora = utilities.load_results('gcn-base-concatenation-standard-norm-cora')\n",
        "print(gcn_base_concatenation_standard_norm_cora)"
      ],
      "metadata": {
        "id": "d6fBnFy_l5b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2: basic GCN + combinations of structural and positional feature + MLP pre-processing layer"
      ],
      "metadata": {
        "id": "s3RtOYGflTCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic models + 128 neurons"
      ],
      "metadata": {
        "id": "01zN-9-R67x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_feature_combinations('gcn-pre-concatenation-without-norm-cora', gcn_pre_factory,[64,128])\n",
        "gcn_pre_concatenation_without_norm_cora = utilities.load_results('gcn-pre-concatenation-without-norm-cora')\n",
        "print(gcn_pre_concatenation_without_norm_cora)"
      ],
      "metadata": {
        "id": "uBqQE9FgX0ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic models + 160 neurons"
      ],
      "metadata": {
        "id": "dTarnT1a7f2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_feature_combinations('gcn-pre-concatenation-without-norm-160-cora', gcn_pre_factory,[64,160])\n",
        "gcn_pre_concatenation_without_norm_160_cora = utilities.load_results('gcn-pre-concatenation-without-norm-160-cora')\n",
        "print(gcn_pre_concatenation_without_norm_160_cora)"
      ],
      "metadata": {
        "id": "CTwvgfp47qBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Std normalized models + 128 neurons"
      ],
      "metadata": {
        "id": "2Dk6XoFg7_mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_feature_combinations('gcn-pre-concatenation-standard-norm-cora', gcn_pre_factory,[64,128], normalization = standard_normalization)\n",
        "gcn_pre_concatenation_standard_norm_cora = utilities.load_results('gcn-pre-concatenation-standard-norm-cora')\n",
        "print(gcn_pre_concatenation_standard_norm_cora)"
      ],
      "metadata": {
        "id": "qCTogftn8Hh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Std normalized models + 160 neurons"
      ],
      "metadata": {
        "id": "DsQkrNYs8spR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_feature_combinations('gcn-pre-concatenation-standard-norm-160-cora', gcn_pre_factory,[64,160], normalization = standard_normalization)\n",
        "gcn_pre_concatenation_standard_norm_160_cora = utilities.load_results('gcn-pre-concatenation-standard-norm-160-cora')\n",
        "print(gcn_pre_concatenation_standard_norm_160_cora)"
      ],
      "metadata": {
        "id": "e9Inqfh-8rkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 3: basic GCN + combinations of structural and positional feature + Ensemble"
      ],
      "metadata": {
        "id": "-yWiPkNElv1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clone = data.clone()\n",
        "data = data.to(device)"
      ],
      "metadata": {
        "id": "ctXRjJ6GUGBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ensemble(data_orig, classifier, scaler, n_runs, file_name, normalization = lambda x: x):\n",
        "\n",
        "  test_accs = []\n",
        "  for i in range(1,n_runs+1):\n",
        "    print(f\"\\n RUN: {i}\\n\")\n",
        "\n",
        "    data = data_orig.clone()\n",
        "\n",
        "    data.val_mask, data.ensemble_val_mask = ensemble.get_val_set_split(data)\n",
        "\n",
        "    data.x = original_features\n",
        "    if data.name!='Cora':\n",
        "      data.x = normalization(data.x)\n",
        "    model_original = gcn.GCNBase(data,hidden_channels=64)\n",
        "    model_original = model_original.to(device)\n",
        "    optimizer = torch.optim.Adam(model_original.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    train_losses, train_accs, val_losses, val_accs, best_epoch = model_training(n_epochs, model_original, data, optimizer, criterion)\n",
        "\n",
        "    print(f\"\\n Model with original features: training completed\\n\")\n",
        "\n",
        "    data.x = positional_features\n",
        "    data.x = normalization(data.x)\n",
        "    model_positional = gcn.GCNBase(data,hidden_channels=64)\n",
        "    model_positional = model_positional.to(device)\n",
        "    optimizer = torch.optim.Adam(model_positional.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    train_losses, train_accs, val_losses, val_accs, best_epoch = model_training(n_epochs, model_positional, data, optimizer, criterion)\n",
        "\n",
        "    print(f\"\\n Model with positional features: training completed\\n\")\n",
        "\n",
        "    models = [model_original, model_positional]\n",
        "    features = [original_features, positional_features]\n",
        "\n",
        "    meta_model_train = ensemble.get_meta_model_features(models, features, data.ensemble_val_mask, data.edge_index)\n",
        "    meta_model_test = ensemble.get_meta_model_features(models, features, data.test_mask, data.edge_index)\n",
        "\n",
        "    X_train = meta_model_train.cpu().numpy()\n",
        "    y_train = data.y[data.ensemble_val_mask].cpu().numpy()\n",
        "    X_test = meta_model_test.cpu().numpy()\n",
        "    y_test = data.y[data.test_mask].cpu().numpy()\n",
        "\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_pred = classifier.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    test_accs.append(accuracy)\n",
        "\n",
        "    print(f\"\\n Ensemble: training completed\")\n",
        "    print(f\"Ensemble accuracy: {accuracy}\")\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "  results = dict()\n",
        "  results['test_accs'] = test_accs\n",
        "  results['avg_acc'] = sum(test_accs) / len(test_accs)\n",
        "\n",
        "  utilities.save_results(results, file_name)"
      ],
      "metadata": {
        "id": "aIlGDiG5H5yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic models as base models"
      ],
      "metadata": {
        "id": "Zphn2ftqE-Cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ],
      "metadata": {
        "id": "-j2vKv6hC7BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = FunctionTransformer(lambda x: x)\n",
        "run_ensemble(data,svm.SVC(verbose=0),scaler,5,'gcn-ensemble-SVM-without-norm-cora')\n",
        "gcn_ensemble_SVM_without_norm_cora = utilities.load_results('gcn-ensemble-SVM-without-norm-cora')\n",
        "print(gcn_ensemble_SVM_without_norm_cora)"
      ],
      "metadata": {
        "id": "XG4rhcAxC-LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM + Std scaler"
      ],
      "metadata": {
        "id": "QGHMhw3m_Fh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "run_ensemble(data,svm.SVC(verbose=0),scaler,5,'gcn-ensemble-SVM-standard-norm-cora')\n",
        "gcn_ensemble_SVM_standard_norm_cora = utilities.load_results('gcn-ensemble-SVM-standard-norm-cora')\n",
        "print(gcn_ensemble_SVM_standard_norm_cora)"
      ],
      "metadata": {
        "id": "dUvRBbz5_B7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regressor"
      ],
      "metadata": {
        "id": "xVIECaxG_fyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = FunctionTransformer(lambda x: x)\n",
        "run_ensemble(data,LogisticRegression(max_iter=10000, multi_class=\"multinomial\",verbose=0),scaler,5,'gcn-ensemble-LR-without-norm-cora')\n",
        "gcn_ensemble_LR_without_norm_cora = utilities.load_results('gcn-ensemble-LR-without-norm-cora')\n",
        "print(gcn_ensemble_LR_without_norm_cora)"
      ],
      "metadata": {
        "id": "VsHtLZoUEYzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regressor + Std"
      ],
      "metadata": {
        "id": "s5PIoCyBhR1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "run_ensemble(data,LogisticRegression(max_iter=10000, multi_class=\"multinomial\",verbose=0),scaler,5,'gcn-ensemble-LR-standard-norm-cora')\n",
        "gcn_ensemble_LR_standard_norm_cora = utilities.load_results('gcn-ensemble-LR-standard-norm-cora')\n",
        "print(gcn_ensemble_LR_standard_norm_cora)"
      ],
      "metadata": {
        "id": "Yj0GSsG5hVEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree"
      ],
      "metadata": {
        "id": "SCDPvxASrJwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = FunctionTransformer(lambda x: x)\n",
        "run_ensemble(data,DecisionTreeClassifier(),scaler,5,'gcn-ensemble-DT-without-norm-cora')\n",
        "gcn_ensemble_DT_without_norm_cora = utilities.load_results('gcn-ensemble-DT-without-norm-cora')\n",
        "print(gcn_ensemble_DT_without_norm_cora)"
      ],
      "metadata": {
        "id": "SfhD8ir3rI6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree + Std"
      ],
      "metadata": {
        "id": "sELlk-narNug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "run_ensemble(data,DecisionTreeClassifier(),scaler,5,'gcn-ensemble-DT-standard-norm-cora')\n",
        "gcn_ensemble_DT_standard_norm_cora = utilities.load_results('gcn-ensemble-DT-standard-norm-cora')\n",
        "print(gcn_ensemble_DT_standard_norm_cora)"
      ],
      "metadata": {
        "id": "NdrQ7DFyrPau",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}