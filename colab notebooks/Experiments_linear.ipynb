{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oISl_AJMCYUl"
      },
      "outputs": [],
      "source": [
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fSVCVnakyty"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "    --extra-index-url=https://pypi.nvidia.com \\\n",
        "    cudf-cu12==23.12.* dask-cudf-cu12==23.12.* cuml-cu12==23.12.* \\\n",
        "    cugraph-cu12==23.12.* cuspatial-cu12==23.12.* cuproj-cu12==23.12.* \\\n",
        "    cuxfilter-cu12==23.12.* cucim-cu12==23.12.* pylibraft-cu12==23.12.* \\\n",
        "    raft-dask-cu12==23.12.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx3P0_wSie3S"
      },
      "outputs": [],
      "source": [
        "from cuml import svm\n",
        "from cuml import LogisticRegression\n",
        "from cuml.common import logger\n",
        "from additional import *\n",
        "from features import *\n",
        "from models import *\n",
        "from models.model import model_training\n",
        "import torch\n",
        "import pickle as pk\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler,FunctionTransformer, Normalizer\n",
        "import numpy\n",
        "import gc\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.base import clone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iRAueiViqeE"
      },
      "outputs": [],
      "source": [
        "# pass 'ogbn-arxiv' to load ArXiv dataset\n",
        "G, data = datasets.load_data('cora')\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRskOYKY6g0E"
      },
      "source": [
        "# Experiment 1: Linear models + combinations of structural and positional features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_fB_lms61zw"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# global features\n",
        "original_features = data.x.to(device)\n",
        "structural_features = features.structural_features(G,['cc', 'bc', 'dc', 'ec', 'pr', 'cn', 'lc', 'nd', 'kc']).to(device)\n",
        "positional_features = features.positional_features(data,128,50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWhH9lJiZxo8"
      },
      "outputs": [],
      "source": [
        "#structural_features=utilities.load_results('structural_features')\n",
        "#positional_features=utilities.load_results('positional_features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngCX4ugLcsQl"
      },
      "outputs": [],
      "source": [
        "#%load_ext autoreload\n",
        "#%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFKhMWRciBgz"
      },
      "outputs": [],
      "source": [
        "def run_feature_combinations(file_name, classifier_original, normalization=lambda x: x):\n",
        "    features_combinations = [\n",
        "      original_features,\n",
        "      structural_features,\n",
        "      positional_features,\n",
        "      utilities.concatenate(original_features,structural_features),\n",
        "      utilities.concatenate(original_features,positional_features),\n",
        "      utilities.concatenate(structural_features,positional_features),\n",
        "      utilities.concatenate(original_features,structural_features,positional_features)]\n",
        "\n",
        "    file_names = [\n",
        "      'original',\n",
        "      'structural',\n",
        "      'positional',\n",
        "      'original-structural',\n",
        "      'original-positional',\n",
        "      'structural-positional',\n",
        "      'original-structural-positional']\n",
        "\n",
        "    basic_models = dict()\n",
        "    orig_num_feat = original_features.size()[1]\n",
        "    for curr_features, curr_file_name in zip(features_combinations, file_names):\n",
        "        classifier = clone(classifier_original)\n",
        "\n",
        "        data.x = curr_features\n",
        "        data.x = normalization(data.x)\n",
        "\n",
        "        if data.name=='Cora' and (curr_file_name=='original' or curr_file_name=='original-structural' or curr_file_name=='original-positional' or curr_file_name=='original-structural-positional'):\n",
        "          split = curr_features.split([orig_num_feat,curr_features.size()[1]-orig_num_feat],dim=-1)\n",
        "          orig_feats = split[0]\n",
        "          other_feats = split[1]\n",
        "          other_feats_norm = normalization(other_feats)\n",
        "          data.x = utilities.concatenate(orig_feats,other_feats_norm)\n",
        "\n",
        "        X_train = data.x[data.train_mask].cpu().numpy()\n",
        "        y_train = data.y[data.train_mask].cpu().numpy()\n",
        "        X_test = data.x[data.test_mask].cpu().numpy()\n",
        "        y_test = data.y[data.test_mask].cpu().numpy()\n",
        "\n",
        "        classifier.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = classifier.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        basic_models[curr_file_name] = {'avg_acc': accuracy}\n",
        "\n",
        "        print(f'Training {curr_file_name} completed!')\n",
        "\n",
        "\n",
        "    utilities.save_results(basic_models, file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEz7QSMhxeR0"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydGwUPambpdP"
      },
      "outputs": [],
      "source": [
        "classifier = LogisticRegression(max_iter=10000, multi_class=\"multinomial\",verbose=0)\n",
        "run_feature_combinations('lr_arxiv', classifier)\n",
        "lr_arxiv = utilities.load_results('lr_arxiv')\n",
        "print(lr_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lbQ-_ktl7oH"
      },
      "source": [
        "## Logistic Regression + Min-Max Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C_C8xekmFVJ"
      },
      "outputs": [],
      "source": [
        "classifier = LogisticRegression(max_iter=10000, multi_class=\"multinomial\",verbose=0)\n",
        "normalization = lambda x : utilities.MinMaxNormalization(x)\n",
        "run_feature_combinations('lr_minmax_arxiv', classifier, normalization)\n",
        "lr_minmax_arxiv = utilities.load_results('lr_minmax_arxiv')\n",
        "print(lr_minmax_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnGXu_uR2JxD"
      },
      "source": [
        "## Logistic Regression + Std Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOnN87yE2NfU"
      },
      "outputs": [],
      "source": [
        "classifier = LogisticRegression(max_iter=10000, multi_class=\"multinomial\",verbose=0)\n",
        "normalization = lambda x : utilities.StandardNormalization(x)\n",
        "run_feature_combinations('lr_std_arxiv', classifier, normalization)\n",
        "lr_std_arxiv = utilities.load_results('lr_std_arxiv')\n",
        "print(lr_std_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2yAiEETxhWt"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88Xo6YioxQDK"
      },
      "outputs": [],
      "source": [
        "classifier = svm.SVC(verbose=0)\n",
        "run_feature_combinations('svm_arxiv', classifier)\n",
        "svm_arxiv = utilities.load_results('svm_arxiv')\n",
        "print(svm_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1coVPYee2hGz"
      },
      "source": [
        "## SVM + Min-Max Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtH70wvD2pj7"
      },
      "outputs": [],
      "source": [
        "classifier = svm.SVC(verbose=0)\n",
        "normalization = lambda x : utilities.MinMaxNormalization(x)\n",
        "run_feature_combinations('svm_minmax_arxiv', classifier, normalization)\n",
        "svm_minmax_arxiv = utilities.load_results('svm_minmax_arxiv')\n",
        "print(svm_minmax_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEhtdeIm2ze1"
      },
      "source": [
        "## SVM + Std Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heImQTsP24NZ"
      },
      "outputs": [],
      "source": [
        "classifier = svm.SVC(verbose=0)\n",
        "normalization = lambda x : utilities.StandardNormalization(x)\n",
        "run_feature_combinations('svm_std_arxiv', classifier, normalization)\n",
        "svm_std_arxiv = utilities.load_results('svm_std_arxiv')\n",
        "print(svm_std_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqk5jlQxsS7"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMgWQ1P4xyZ0"
      },
      "outputs": [],
      "source": [
        "classifier = DecisionTreeClassifier(random_state=404)\n",
        "run_feature_combinations('dt_arxiv', classifier)\n",
        "dt_arxiv = utilities.load_results('dt_arxiv')\n",
        "print(dt_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsflO7Kb3bfw"
      },
      "source": [
        "## Decision Tree + Min-Max Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi00XWOt3g4i"
      },
      "outputs": [],
      "source": [
        "classifier = DecisionTreeClassifier(random_state=404)\n",
        "normalization = lambda x : utilities.MinMaxNormalization(x)\n",
        "run_feature_combinations('dt_minmax_arxiv', classifier, normalization)\n",
        "dt_minmax_arxiv = utilities.load_results('dt_minmax_arxiv')\n",
        "print(dt_minmax_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj47vtDh3soH"
      },
      "source": [
        "## Decision Tree + Std Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3Guw8da3wyb"
      },
      "outputs": [],
      "source": [
        "classifier = DecisionTreeClassifier(random_state=404)\n",
        "normalization = lambda x : utilities.StandardNormalization(x)\n",
        "run_feature_combinations('dt_std_arxiv', classifier, normalization)\n",
        "dt_std_arxiv = utilities.load_results('dt_std_arxiv')\n",
        "print(dt_std_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yWiPkNElv1c"
      },
      "source": [
        "## Experiment 2: linear models + combinations of structural and positional feature + Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIlGDiG5H5yo"
      },
      "outputs": [],
      "source": [
        "def run_ensemble(data_orig, classifier_meta, scaler_meta, classifier_base, file_name):\n",
        "\n",
        "  data = data_orig.clone()\n",
        "  #data.val_mask, data.ensemble_val_mask = ensemble.get_val_set_split(data)\n",
        "\n",
        "  features = [original_features, positional_features]\n",
        "  models = []\n",
        "  for feat in features:\n",
        "    cl = clone(classifier_base)\n",
        "    data.x = feat\n",
        "\n",
        "    X_train = data.x[data.train_mask].cpu().numpy()\n",
        "    y_train = data.y[data.train_mask].cpu().numpy()\n",
        "    X_test = data.x[data.test_mask].cpu().numpy()\n",
        "    y_test = data.y[data.test_mask].cpu().numpy()\n",
        "\n",
        "    cl.fit(X_train, y_train)\n",
        "\n",
        "    models.append(cl)\n",
        "\n",
        "    del cl\n",
        "    gc.collect()\n",
        "\n",
        "  meta_model_train = ensemble.get_meta_model_features(models, features, data.val_mask, data.edge_index,linear=True)\n",
        "  meta_model_test = ensemble.get_meta_model_features(models, features, data.test_mask, data.edge_index,linear=True)\n",
        "\n",
        "  del models\n",
        "  gc.collect()\n",
        "\n",
        "  X_train = meta_model_train.cpu().numpy()\n",
        "  y_train = data.y[data.val_mask].cpu().numpy()\n",
        "  X_test = meta_model_test.cpu().numpy()\n",
        "  y_test = data.y[data.test_mask].cpu().numpy()\n",
        "\n",
        "  X_train_scaled = scaler_meta.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_meta.transform(X_test)\n",
        "\n",
        "  classifier_meta.fit(X_train_scaled, y_train)\n",
        "\n",
        "  y_pred = classifier_meta.predict(X_test_scaled)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  print(f\"Ensemble training completed\")\n",
        "  print(f\"Ensemble accuracy: {accuracy}\")\n",
        "\n",
        "  results = dict()\n",
        "  results['avg_acc'] = accuracy\n",
        "\n",
        "  utilities.save_results(results, file_name)\n",
        "\n",
        "  del classifier_meta\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr_B5DHl6fKx"
      },
      "source": [
        "## SVM as meta and DT as base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLk2HlbG6eTk"
      },
      "outputs": [],
      "source": [
        "classifier_meta = svm.SVC(verbose=0)\n",
        "classifier_base = DecisionTreeClassifier(random_state=404)\n",
        "scaler_meta = FunctionTransformer(lambda x: x)\n",
        "\n",
        "run_ensemble(data, classifier_meta, scaler_meta, classifier_base, 'ensemble_svm_dt_arxiv')\n",
        "ensemble_svm_dt_arxiv = utilities.load_results('ensemble_svm_dt_arxiv')\n",
        "print(ensemble_svm_dt_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLR8wLuqOLr2"
      },
      "source": [
        "## SVM as meta and LR as base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rhCDS0EOOm9"
      },
      "outputs": [],
      "source": [
        "classifier_meta = svm.SVC(verbose=0)\n",
        "classifier_base = LogisticRegression(max_iter=10000, multi_class=\"multinomial\",verbose=0)\n",
        "scaler_meta = FunctionTransformer(lambda x: x)\n",
        "\n",
        "run_ensemble(data, classifier_meta, scaler_meta, classifier_base, 'ensemble_svm_lr_arxiv')\n",
        "ensemble_svm_lr_arxiv = utilities.load_results('ensemble_svm_lr_arxiv')\n",
        "print(ensemble_svm_lr_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlpYPXFZ_9Sj"
      },
      "source": [
        "## LR as meta and DT as base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5x5sAmWAA-1"
      },
      "outputs": [],
      "source": [
        "classifier_meta = LogisticRegression(max_iter=10000, multi_class=\"multinomial\",verbose=0)\n",
        "classifier_base = DecisionTreeClassifier(random_state=404)\n",
        "scaler_meta = FunctionTransformer(lambda x: x)\n",
        "\n",
        "run_ensemble(data, classifier_meta, scaler_meta, classifier_base, 'ensemble_lr_dt_arxiv')\n",
        "ensemble_lr_dt_arxiv = utilities.load_results('ensemble_lr_dt_arxiv')\n",
        "print(ensemble_lr_dt_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poy4I5p1ApoQ"
      },
      "source": [
        "## LR as meta and SVM as base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8T1TuCIArlv"
      },
      "outputs": [],
      "source": [
        "classifier_meta = LogisticRegression(max_iter=10000, multi_class=\"multinomial\",verbose=0)\n",
        "classifier_base = svm.SVC(verbose=0)\n",
        "scaler_meta = FunctionTransformer(lambda x: x)\n",
        "\n",
        "run_ensemble(data, classifier_meta, scaler_meta, classifier_base, 'ensemble_lr_svm_arxiv')\n",
        "ensemble_lr_svm_arxiv = utilities.load_results('ensemble_lr_svm_arxiv')\n",
        "print(ensemble_lr_svm_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLzm-jxjyC9u"
      },
      "source": [
        "## LR + min-max as meta and SVM as base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-3Yve9HyB3b"
      },
      "outputs": [],
      "source": [
        "classifier_meta = LogisticRegression(max_iter=10000, multi_class=\"multinomial\",verbose=0)\n",
        "classifier_base = svm.SVC(verbose=0)\n",
        "scaler_meta = MinMaxScaler()\n",
        "\n",
        "run_ensemble(data, classifier_meta, scaler_meta, classifier_base, 'ensemble_lr_minmax_svm_arxiv')\n",
        "ensemble_lr_minmax_svm_arxiv = utilities.load_results('ensemble_lr_minmax_svm_arxiv')\n",
        "print(ensemble_lr_minmax_svm_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66QoiKnAA5ox"
      },
      "source": [
        "## DT as meta and LR as base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moMq36NMA-KW"
      },
      "outputs": [],
      "source": [
        "classifier_meta = DecisionTreeClassifier(random_state=404)\n",
        "classifier_base = LogisticRegression(max_iter=10000, multi_class=\"multinomial\",verbose=0)\n",
        "scaler_meta = FunctionTransformer(lambda x: x)\n",
        "\n",
        "run_ensemble(data, classifier_meta, scaler_meta, classifier_base, 'ensemble_dt_lr_arxiv')\n",
        "ensemble_dt_lr_arxiv = utilities.load_results('ensemble_dt_lr_arxiv')\n",
        "print(ensemble_dt_lr_arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEJc0CvzBRc8"
      },
      "source": [
        "## DT as meta and SVM as base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkCdE8R2BTop"
      },
      "outputs": [],
      "source": [
        "classifier_meta = DecisionTreeClassifier(random_state=404)\n",
        "classifier_base = svm.SVC(verbose=0)\n",
        "scaler_meta = FunctionTransformer(lambda x: x)\n",
        "\n",
        "run_ensemble(data, classifier_meta, scaler_meta, classifier_base, 'ensemble_dt_svm_arxiv')\n",
        "ensemble_dt_svm_arxiv = utilities.load_results('ensemble_dt_svm_arxiv')\n",
        "print(ensemble_dt_svm_arxiv)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
